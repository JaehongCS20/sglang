usage: launch_server.py [-h] --model-path MODEL_PATH [--tokenizer-path TOKENIZER_PATH] [--host HOST] [--port PORT] [--tokenizer-mode {auto,slow}] [--load-format {auto,pt,safetensors,npcache,dummy,gguf,bitsandbytes}] [--trust-remote-code]
                        [--dtype {auto,half,float16,bfloat16,float,float32}] [--kv-cache-dtype {auto,fp8_e5m2}] [--quantization {awq,fp8,gptq,marlin,gptq_marlin,awq_marlin,bitsandbytes,gguf,modelopt}] [--context-length CONTEXT_LENGTH] [--device {cuda,xpu,hpu}]
                        [--served-model-name SERVED_MODEL_NAME] [--chat-template CHAT_TEMPLATE] [--is-embedding] [--revision REVISION] [--skip-tokenizer-init] [--return-token-ids] [--mem-fraction-static MEM_FRACTION_STATIC]
                        [--max-running-requests MAX_RUNNING_REQUESTS] [--max-total-tokens MAX_TOTAL_TOKENS] [--chunked-prefill-size CHUNKED_PREFILL_SIZE] [--max-prefill-tokens MAX_PREFILL_TOKENS] [--schedule-policy {lpm,random,fcfs,dfs-weight}]
                        [--schedule-conservativeness SCHEDULE_CONSERVATIVENESS] [--cpu-offload-gb CPU_OFFLOAD_GB] [--prefill-only-one-req PREFILL_ONLY_ONE_REQ] [--tensor-parallel-size TENSOR_PARALLEL_SIZE] [--stream-interval STREAM_INTERVAL]
                        [--random-seed RANDOM_SEED] [--constrained-json-whitespace-pattern CONSTRAINED_JSON_WHITESPACE_PATTERN] [--watchdog-timeout WATCHDOG_TIMEOUT] [--download-dir DOWNLOAD_DIR] [--base-gpu-id BASE_GPU_ID] [--log-level LOG_LEVEL]
                        [--log-level-http LOG_LEVEL_HTTP] [--log-requests] [--show-time-cost] [--enable-metrics] [--decode-log-interval DECODE_LOG_INTERVAL] [--api-key API_KEY] [--file-storage-pth FILE_STORAGE_PTH] [--enable-cache-report]
                        [--data-parallel-size DATA_PARALLEL_SIZE] [--load-balance-method {round_robin,shortest_queue}] [--expert-parallel-size EXPERT_PARALLEL_SIZE] [--dist-init-addr DIST_INIT_ADDR] [--nnodes NNODES] [--node-rank NODE_RANK]
                        [--json-model-override-args JSON_MODEL_OVERRIDE_ARGS] [--lora-paths [LORA_PATHS ...]] [--max-loras-per-batch MAX_LORAS_PER_BATCH] [--attention-backend {flashinfer,triton,torch_native}] [--sampling-backend {flashinfer,pytorch}]
                        [--grammar-backend {xgrammar,outlines}] [--speculative-algorithm {EAGLE}] [--speculative-draft-model-path SPECULATIVE_DRAFT_MODEL_PATH] [--speculative-num-steps SPECULATIVE_NUM_STEPS]
                        [--speculative-num-draft-tokens SPECULATIVE_NUM_DRAFT_TOKENS] [--speculative-eagle-topk {1,2,4,8}] [--enable-double-sparsity] [--ds-channel-config-path DS_CHANNEL_CONFIG_PATH] [--ds-heavy-channel-num DS_HEAVY_CHANNEL_NUM]
                        [--ds-heavy-token-num DS_HEAVY_TOKEN_NUM] [--ds-heavy-channel-type DS_HEAVY_CHANNEL_TYPE] [--ds-sparse-decode-threshold DS_SPARSE_DECODE_THRESHOLD] [--disable-radix-cache] [--disable-jump-forward] [--disable-cuda-graph]
                        [--disable-cuda-graph-padding] [--disable-outlines-disk-cache] [--disable-custom-all-reduce] [--disable-mla] [--disable-overlap-schedule] [--enable-mixed-chunk] [--enable-dp-attention] [--enable-ep-moe] [--enable-torch-compile]
                        [--torch-compile-max-bs TORCH_COMPILE_MAX_BS] [--cuda-graph-max-bs CUDA_GRAPH_MAX_BS] [--cuda-graph-bs CUDA_GRAPH_BS [CUDA_GRAPH_BS ...]] [--torchao-config TORCHAO_CONFIG] [--enable-nan-detection] [--enable-p2p-check]
                        [--triton-attention-reduce-in-fp32] [--triton-attention-num-kv-splits TRITON_ATTENTION_NUM_KV_SPLITS] [--num-continuous-decode-steps NUM_CONTINUOUS_DECODE_STEPS] [--delete-ckpt-after-loading]

  -h, --help            show this help message and exit
  --model-path MODEL_PATH
                        The path of the model weights. This can be a local folder or a Hugging Face repo ID.
  --tokenizer-path TOKENIZER_PATH
                        The path of the tokenizer.
  --host HOST           The host of the server.
  --port PORT           The port of the server.
  --tokenizer-mode {auto,slow}
                        Tokenizer mode. 'auto' will use the fast tokenizer if available, and 'slow' will always use the slow tokenizer.
  --load-format {auto,pt,safetensors,npcache,dummy,gguf,bitsandbytes}
                        The format of the model weights to load. "auto" will try to load the weights in the safetensors format and fall back to the pytorch bin format if safetensors format is not available. "pt" will load the weights in the pytorch bin format.
                        "safetensors" will load the weights in the safetensors format. "npcache" will load the weights in pytorch format and store a numpy cache to speed up the loading. "dummy" will initialize the weights with random values, which is mainly for
                        profiling."gguf" will load the weights in the gguf format. "bitsandbytes" will load the weights using bitsandbytes quantization.
  --trust-remote-code   Whether or not to allow for custom models defined on the Hub in their own modeling files.
  --dtype {auto,half,float16,bfloat16,float,float32}
                        Data type for model weights and activations. * "auto" will use FP16 precision for FP32 and FP16 models, and BF16 precision for BF16 models. * "half" for FP16. Recommended for AWQ quantization. * "float16" is the same as "half". *
                        "bfloat16" for a balance between precision and range. * "float" is shorthand for FP32 precision. * "float32" for FP32 precision.
  --kv-cache-dtype {auto,fp8_e5m2}
                        Data type for kv cache storage. "auto" will use model data type. "fp8_e5m2" is supported for CUDA 11.8+.
  --quantization {awq,fp8,gptq,marlin,gptq_marlin,awq_marlin,bitsandbytes,gguf,modelopt}
                        The quantization method.
  --context-length CONTEXT_LENGTH
                        The model's maximum context length. Defaults to None (will use the value from the model's config.json instead).
  --device {cuda,xpu,hpu}
                        The device type.
  --served-model-name SERVED_MODEL_NAME
                        Override the model name returned by the v1/models endpoint in OpenAI API server.
  --chat-template CHAT_TEMPLATE
                        The buliltin chat template name or the path of the chat template file. This is only used for OpenAI-compatible API server.
  --is-embedding        Whether to use a CausalLM as an embedding model.
  --revision REVISION   The specific model version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.
  --skip-tokenizer-init
                        If set, skip init tokenizer and pass input_ids in generate request
  --return-token-ids    Whether to return token IDs in the output, this may introduce additional overhead.
  --mem-fraction-static MEM_FRACTION_STATIC
                        The fraction of the memory used for static allocation (model weights and KV cache memory pool). Use a smaller value if you see out-of-memory errors.
  --max-running-requests MAX_RUNNING_REQUESTS
                        The maximum number of running requests.
  --max-total-tokens MAX_TOTAL_TOKENS
                        The maximum number of tokens in the memory pool. If not specified, it will be automatically calculated based on the memory usage fraction. This option is typically used for development and debugging purposes.
  --chunked-prefill-size CHUNKED_PREFILL_SIZE
                        The maximum number of tokens in a chunk for the chunked prefill. Setting this to -1 means disabling chunked prefill
  --max-prefill-tokens MAX_PREFILL_TOKENS
                        The maximum number of tokens in a prefill batch. The real bound will be the maximum of this value and the model's maximum context length.
  --schedule-policy {lpm,random,fcfs,dfs-weight}
                        The scheduling policy of the requests.
  --schedule-conservativeness SCHEDULE_CONSERVATIVENESS
                        How conservative the schedule policy is. A larger value means more conservative scheduling. Use a larger value if you see requests being retracted frequently.
  --cpu-offload-gb CPU_OFFLOAD_GB
                        How many GBs of RAM to reserve for CPU offloading
  --prefill-only-one-req PREFILL_ONLY_ONE_REQ
                        If true, we only prefill one request at one prefill batch
  --tensor-parallel-size TENSOR_PARALLEL_SIZE, --tp-size TENSOR_PARALLEL_SIZE
                        The tensor parallelism size.
  --stream-interval STREAM_INTERVAL
                        The interval (or buffer size) for streaming in terms of the token length. A smaller value makes streaming smoother, while a larger value makes the throughput higher
  --random-seed RANDOM_SEED
                        The random seed.
  --constrained-json-whitespace-pattern CONSTRAINED_JSON_WHITESPACE_PATTERN
                        Regex pattern for syntactic whitespaces allowed in JSON constrained output. For example, to allow the model generate consecutive whitespaces, set the pattern to [\n\t ]*
  --watchdog-timeout WATCHDOG_TIMEOUT
                        Set watchdog timeout in seconds. If a forward batch takes longer than this, the server will crash to prevent hanging.
  --download-dir DOWNLOAD_DIR
                        Model download directory.
  --base-gpu-id BASE_GPU_ID
                        The base GPU ID to start allocating GPUs from. Useful when running multiple instances on the same machine.
  --log-level LOG_LEVEL
                        The logging level of all loggers.
  --log-level-http LOG_LEVEL_HTTP
                        The logging level of HTTP server. If not set, reuse --log-level by default.
  --log-requests        Log the inputs and outputs of all requests.
  --show-time-cost      Show time cost of custom marks.
  --enable-metrics      Enable log prometheus metrics.
  --decode-log-interval DECODE_LOG_INTERVAL
                        The log interval of decode batch
  --api-key API_KEY     Set API key of the server. It is also used in the OpenAI API compatible server.
  --file-storage-pth FILE_STORAGE_PTH
                        The path of the file storage in backend.
  --enable-cache-report
                        Return number of cached tokens in usage.prompt_tokens_details for each openai request.
  --data-parallel-size DATA_PARALLEL_SIZE, --dp-size DATA_PARALLEL_SIZE
                        The data parallelism size.
  --load-balance-method {round_robin,shortest_queue}
                        The load balancing strategy for data parallelism.
  --expert-parallel-size EXPERT_PARALLEL_SIZE, --ep-size EXPERT_PARALLEL_SIZE
                        The expert parallelism size.
  --dist-init-addr DIST_INIT_ADDR, --nccl-init-addr DIST_INIT_ADDR
                        The host address for initializing distributed backend (e.g., `192.168.0.2:25000`).
  --nnodes NNODES       The number of nodes.
  --node-rank NODE_RANK
                        The node rank.
  --json-model-override-args JSON_MODEL_OVERRIDE_ARGS
                        A dictionary in JSON string format used to override default model configurations.
  --lora-paths [LORA_PATHS ...]
                        The list of LoRA adapters. You can provide a list of either path in str or renamed path in the format {name}={path}
  --max-loras-per-batch MAX_LORAS_PER_BATCH
                        Maximum number of adapters for a running batch, include base-only request
  --attention-backend {flashinfer,triton,torch_native}
                        Choose the kernels for attention layers.
  --sampling-backend {flashinfer,pytorch}
                        Choose the kernels for sampling layers.
  --grammar-backend {xgrammar,outlines}
                        Choose the backend for grammar-guided decoding.
  --speculative-algorithm {EAGLE}
                        Speculative algorithm.
  --speculative-draft-model-path SPECULATIVE_DRAFT_MODEL_PATH
                        The path of the draft model weights. This can be a local folder or a Hugging Face repo ID.
  --speculative-num-steps SPECULATIVE_NUM_STEPS
                        The number of steps sampled from draft model in Speculative Decoding.
  --speculative-num-draft-tokens SPECULATIVE_NUM_DRAFT_TOKENS
                        The number of token sampled from draft model in Speculative Decoding.
  --speculative-eagle-topk {1,2,4,8}
                        The number of token sampled from draft model in eagle2 each step.
  --enable-double-sparsity
                        Enable double sparsity attention
  --ds-channel-config-path DS_CHANNEL_CONFIG_PATH
                        The path of the double sparsity channel config
  --ds-heavy-channel-num DS_HEAVY_CHANNEL_NUM
                        The number of heavy channels in double sparsity attention
  --ds-heavy-token-num DS_HEAVY_TOKEN_NUM
                        The number of heavy tokens in double sparsity attention
  --ds-heavy-channel-type DS_HEAVY_CHANNEL_TYPE
                        The type of heavy channels in double sparsity attention
  --ds-sparse-decode-threshold DS_SPARSE_DECODE_THRESHOLD
                        The type of heavy channels in double sparsity attention
  --disable-radix-cache
                        Disable RadixAttention for prefix caching.
  --disable-jump-forward
                        Disable jump-forward for grammar-guided decoding.
  --disable-cuda-graph  Disable cuda graph.
  --disable-cuda-graph-padding
                        Disable cuda graph when padding is needed. Still uses cuda graph when padding is not needed.
  --disable-outlines-disk-cache
                        Disable disk cache of outlines to avoid possible crashes related to file system or high concurrency.
  --disable-custom-all-reduce
                        Disable the custom all-reduce kernel and fall back to NCCL.
  --disable-mla         Disable Multi-head Latent Attention (MLA) for DeepSeek-V2.
  --disable-overlap-schedule
                        Disable the overlap scheduler, which overlaps the CPU scheduler with GPU model worker.
  --enable-mixed-chunk  Enabling mixing prefill and decode in a batch when using chunked prefill.
  --enable-dp-attention
                        Enabling data parallelism for attention and tensor parallelism for FFN. The dp size should be equal to the tp size. Currently only DeepSeek-V2 is supported.
  --enable-ep-moe       Enabling expert parallelism for moe. The ep size is equal to the tp size.
  --enable-torch-compile
                        Optimize the model with torch.compile. Experimental feature.
  --torch-compile-max-bs TORCH_COMPILE_MAX_BS
                        Set the maximum batch size when using torch compile.
  --cuda-graph-max-bs CUDA_GRAPH_MAX_BS
                        Set the maximum batch size for cuda graph.
  --cuda-graph-bs CUDA_GRAPH_BS [CUDA_GRAPH_BS ...]
                        Set the list of batch sizes for cuda graph.
  --torchao-config TORCHAO_CONFIG
                        Optimize the model with torchao. Experimental feature. Current choices are: int8dq, int8wo, int4wo-<group_size>, fp8wo, fp8dq-per_tensor, fp8dq-per_row
  --enable-nan-detection
                        Enable the NaN detection for debugging purposes.
  --enable-p2p-check    Enable P2P check for GPU access, otherwise the p2p access is allowed by default.
  --triton-attention-reduce-in-fp32
                        Cast the intermidiate attention results to fp32 to avoid possible crashes related to fp16.This only affects Triton attention kernels.
  --triton-attention-num-kv-splits TRITON_ATTENTION_NUM_KV_SPLITS
                        The number of KV splits in flash decoding Triton kernel. Larger value is better in longer context scenarios. The default value is 8.
  --num-continuous-decode-steps NUM_CONTINUOUS_DECODE_STEPS
                        Run multiple continuous decoding steps to reduce scheduling overhead. This can potentially increase throughput but may also increase time-to-first-token latency. The default value is 1, meaning only run one decoding step at a time.
  --delete-ckpt-after-loading
                        Delete the model checkpoint after loading the model.